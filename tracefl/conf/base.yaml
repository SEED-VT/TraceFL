defaults:
  - constants # Includes all values from the file "constants.yaml"
  - _self_ # Ensures this fileâ€™s own contents are loaded last so they can override if necessary

exp_key: "Temp-" # Experiment key or name. Useful for uniquely identifying experiment runs. # This is a placeholder and typically overwritten when the final experiment runs.

num_clients: 20 # Total number of federated clients.
clients_per_round: 4 # Number of clients selected per federated round.
num_rounds: 3 # Number of total federated rounds.

dirichlet_alpha: 0.3 # Alpha parameter for the Dirichlet distribution # controlling non-IID data splits among clients.

batch_size: 32 # Mini-batch size used by each client during training.

# Differential Privacy Settings
noise_multiplier: -1 # If > 0, indicates the noise multiplier for DP;  # -1 or 0 typically indicates DP is disabled.
clipping_norm: -1 # If > 0, maximum gradient norm (clipping) for DP; # -1 indicates no clipping.

################################################################################
# Device and Resource Configuration
################################################################################
device: cpu # Execution device: "cpu" or "cuda".
total_gpus: 1 # Total number of GPUs available (especially relevant if device="cuda").
total_cpus: 2 # Total number of CPU cores available.

client_resources:
  cpus: 1 # CPU resources allocated per client (for scheduling or parallelization).
  gpus: 1 # Fraction of GPU allocated per client. Only effective if device="cuda".

################################################################################
# Client Configuration
################################################################################
client:
  epochs: 2 # Number of local epochs each client performs per round.
  lr: 0.001 # Learning rate used by each client.

################################################################################
# Model Configuration
################################################################################
model:
  name: "resnet18" # Name of the model to be used.
  arch: ${model_arch.${model.name}} # Uses interpolation from "constants.yaml" (e.g., "cnn" or "transformer").

################################################################################
# Dataset Configuration
################################################################################
dataset:
  name: "mnist" # Name of the dataset (e.g., "mnist", "cifar10", "organamnist", etc.).
  num_classes: ${dataset_classes.${dataset.name}} # Dynamically fetched from constants.yaml.
  channels: ${dataset_channels.${dataset.name}} # Dynamically fetched from constants.yaml.

################################################################################
# Federated Strategy Configuration
################################################################################
strategy:
  name: fedavg # Name of the FL strategy (e.g., "fedavg", "fedprox", etc.).
  num_rounds: ${num_rounds} # Referencing the top-level num_rounds.
  clients_per_round: ${clients_per_round}
  noise_multiplier: ${noise_multiplier}
  clipping_norm: ${clipping_norm}

################################################################################
# Data Distribution Configuration
################################################################################
data_dist:
  dist_type: non_iid_dirichlet # Defines how data is split among clients ("non_iid_dirichlet", "PathologicalPartitioner-#").
  num_clients: ${num_clients}
  batch_size: ${batch_size}
  dirichlet_alpha: ${dirichlet_alpha}
  dname: ${dataset.name} # Dataset name, used for naming or caching.
  mname: ${model.name} # Model name, used for naming or caching.
  storage_dir: ${storage.dir} # Base storage directory from constants.yaml.
  max_per_client_data_size: 2048 # Optional limit on how much data each client receives.
  max_server_data_size: 2048 # Optional limit on how much data is stored on the server.
  architecture: ${model.arch} # Model architecture, used for naming or caching.
################################################################################
# Experiment Flags
################################################################################
do_full_cache_provenance: false # If True, enables full caching of provenance data.
dry_run: false # If True, does not execute the full experiment (useful for debugging).
do_training: true # If True, trains the model in a federated manner.
do_provenance: true # If True, runs provenance tracking or auditing.
convert_cache_to_csv: true # If True, converts cache results to CSV format for analysis.
enable_beta: true # Enables beta features or modules if they exist in your code.
plotting: true # If True, generates plots or visualizations at the end of the experiment.

# ################################################################################
# # Hydra Logging Configuration
# ################################################################################
# hydra:
#     job_logging:
#         root:
#             level: INFO # Top-level logging level.
#         loggers:
#             flwr:
#                 level: INFO # Logging level for Flower FL library.
#             accelerate.utils.other:
#                 level: ERROR # Logging level for the Accelerate library (e.g., to suppress unnecessary logs).

# conf.yaml (example)
################################################################################
# Hydra Logging Configuration
################################################################################
hydra:
  job_logging:
    version: 1
    disable_existing_loggers: false

    formatters:
      simple:
        # The format string below omits time (%(asctime)s) and logger name (%(name)s)
        # so you'll only see the level, a dash, and the actual message.
        format: "[%(levelname)s] - %(message)s"

    handlers:
      console:
        class: logging.StreamHandler
        formatter: simple

      file:
        class: logging.FileHandler
        formatter: simple
        filename: TraceFL_clients_contributions.log
        # Optional: decide append vs overwrite
        mode: w # 'a' for append, 'w' for overwrite

    root:
      level: INFO
      handlers: [console, file]

    loggers:
      flwr:
        level: INFO
        handlers: [console, file]
        propagate: false

      accelerate.utils.other:
        level: ERROR
        handlers: [console]
        propagate: false
